"""
Pipeline Runner - Menu Processor v2
ãƒ¡ãƒ‹ãƒ¥ãƒ¼å‡¦ç†ã®åŸºæœ¬ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ: OCR â†’ Mapping â†’ Categorize â†’ DB Save â†’ SSE â†’ Parallel Tasks
"""
import time
import json
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime

from app_2.services.ocr_service import get_ocr_service
from app_2.services.categorize_service import get_categorize_service
from app_2.services.mapping_service import get_menu_mapping_categorize_service
from app_2.services.menu_save_service import create_menu_save_service
from app_2.services.dependencies import get_menu_repository, get_session_repository
from app_2.infrastructure.integrations.redis.redis_publisher import RedisPublisher
from app_2.domain.entities.session_entity import SessionEntity, SessionStatus
from app_2.utils.logger import get_logger

logger = get_logger("pipeline_runner")


class MenuProcessingPipeline:
    """
    ãƒ¡ãƒ‹ãƒ¥ãƒ¼å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    è²¬ä»»:
    1. OCR â†’ Mapping â†’ Categorize â†’ DB Save (4æ®µéšåŸºæœ¬å‡¦ç†)
    2. SSEé€²æ—é…ä¿¡
    3. ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®ãƒˆãƒªã‚¬ãƒ¼
    """
    
    def __init__(self):
        self.redis_publisher = RedisPublisher()
        self.ocr_service = get_ocr_service()
        self.categorize_service = get_categorize_service()
        self.mapping_service = get_menu_mapping_categorize_service()

    async def _update_session_stage_completion(
        self, 
        session_id: str, 
        stage: str, 
        stage_data: Dict[str, Any]
    ) -> bool:
        """
        æ®µéšå®Œäº†æ™‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            stage: å®Œäº†ã—ãŸæ®µéšå
            stage_data: æ®µéšãƒ‡ãƒ¼ã‚¿
            
        Returns:
            bool: æ›´æ–°ãŒæˆåŠŸã—ãŸã‹
        """
        try:
            from app_2.core.database import async_session_factory
            
            async with async_session_factory() as db_session:
                session_repo = get_session_repository(db_session)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å–å¾—
                from sqlalchemy import select
                from app_2.infrastructure.models.session_model import SessionModel
                
                stmt = select(SessionModel).where(SessionModel.session_id == session_id)
                result = await db_session.execute(stmt)
                session_model = result.scalar_one_or_none()
                
                if session_model:
                    # æ®µéšãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                    session_model.update_stage_data(stage, stage_data)
                    await db_session.commit()
                    
                    logger.info(f"âœ… Session {session_id} stage updated: {stage}")
                    return True
                else:
                    logger.error(f"âŒ Session not found for stage update: {session_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ Failed to update session stage {session_id}/{stage}: {e}")
            return False

    async def _execute_ocr_stage(self, session_id: str, image_data: bytes) -> List[Dict]:
        """
        Stage 1: OCRå®Ÿè¡Œ â†’ DBæ›´æ–° â†’ SSEé…ä¿¡
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            image_data: ç”»åƒãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[Dict]: OCRçµæœãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ” Starting OCR stage for session: {session_id}")
        
        # OCRå‡¦ç†é–‹å§‹é€šçŸ¥
        await self._update_progress(session_id, "ocr", "processing", 10)
        
        try:
            # OCRå®Ÿè¡Œ
            ocr_results = await self.ocr_service.extract_text_with_positions(
                image_data, level="paragraph"
            )
            
            logger.info(f"ğŸ“ OCR completed: {len(ocr_results)} text elements extracted")
            
            # ğŸ¯ DBæ›´æ–°: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«OCRçµæœã‚’ä¿å­˜
            stage_data = {
                "ocr_elements_count": len(ocr_results),
                "ocr_results": ocr_results,
                "stage_completed_at": datetime.utcnow().isoformat(),
                "processing_duration": time.time() - time.time(),  # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—ã™ã‚‹å ´åˆ
                "image_analysis": {
                    "text_density": "high" if len(ocr_results) > 20 else "medium" if len(ocr_results) > 10 else "low",
                    "elements_extracted": len(ocr_results),
                    "preview_available": len(ocr_results) > 0
                }
            }
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
            db_update_success = await self._update_session_stage_completion(
                session_id, "ocr_completed", stage_data
            )
            
            if not db_update_success:
                logger.warning(f"âš ï¸ DB update failed for OCR stage, but continuing with SSE broadcast")
            
            # ğŸ¯ SSEé…ä¿¡: OCRå®Œäº†é€šçŸ¥ï¼ˆæ±ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            sse_success = await self.redis_publisher.publish_ocr_completion(
                session_id=session_id,
                ocr_results=ocr_results,
                db_saved=db_update_success
            )
            
            if sse_success:
                logger.info(f"ğŸ“¡ OCR completion broadcasted successfully for session: {session_id}")
            else:
                logger.warning(f"âš ï¸ SSE broadcast failed for OCR completion: {session_id}")
            
            # é€²æ—æ›´æ–°
            await self._update_progress(session_id, "ocr", "completed", 25)
            
            return ocr_results
            
        except Exception as e:
            logger.error(f"âŒ OCR stage failed for session {session_id}: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®SSEé…ä¿¡
            await self.redis_publisher.publish_error_message(
                session_id=session_id,
                error_type="ocr_processing_failed",
                error_message=str(e),
                task_name="ocr"
            )
            
            raise

    async def _execute_mapping_stage(self, session_id: str, ocr_results: List[Dict]) -> str:
        """
        Stage 2: Mappingå®Ÿè¡Œ â†’ DBæ›´æ–° â†’ SSEé…ä¿¡
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            ocr_results: OCRçµæœãƒªã‚¹ãƒˆ
            
        Returns:
            str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        """
        logger.info(f"ğŸ—ºï¸ Starting Mapping stage for session: {session_id}")
        
        await self._update_progress(session_id, "mapping", "processing", 35)
        
        try:
            # ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†å®Ÿè¡Œ
            formatted_mapping_data = self.mapping_service._format_mapping_data(ocr_results)
            
            logger.info("ğŸ“‹ Mapping completed: Position data formatted for categorization")
            
            # ğŸ¯ DBæ›´æ–°: ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’ä¿å­˜
            stage_data = {
                "formatted_data_length": len(formatted_mapping_data),
                "mapping_preview": formatted_mapping_data[:500],
                "stage_completed_at": datetime.utcnow().isoformat(),
                "ocr_elements_processed": len(ocr_results),
                "mapping_analysis": {
                    "data_size": len(formatted_mapping_data),
                    "processing_successful": True,
                    "preview_available": True
                }
            }
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
            db_update_success = await self._update_session_stage_completion(
                session_id, "mapping_completed", stage_data
            )
            
            # ğŸ¯ SSEé…ä¿¡: Mappingå®Œäº†é€šçŸ¥ï¼ˆæ±ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            sse_success = await self.redis_publisher.publish_mapping_completion(
                session_id=session_id,
                mapping_data=formatted_mapping_data,
                db_saved=db_update_success
            )
            
            if sse_success:
                logger.info(f"ğŸ“¡ Mapping completion broadcasted successfully for session: {session_id}")
            else:
                logger.warning(f"âš ï¸ SSE broadcast failed for mapping completion: {session_id}")
            
            await self._update_progress(session_id, "mapping", "completed", 45)
            
            return formatted_mapping_data
            
        except Exception as e:
            logger.error(f"âŒ Mapping stage failed for session {session_id}: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®SSEé…ä¿¡
            await self.redis_publisher.publish_error_message(
                session_id=session_id,
                error_type="mapping_processing_failed",
                error_message=str(e),
                task_name="mapping"
            )
            
            raise

    async def _execute_categorize_stage(self, session_id: str, mapping_data: str) -> Dict[str, Any]:
        """
        Stage 3: Categorizeå®Ÿè¡Œ â†’ DBæ›´æ–° â†’ SSEé…ä¿¡
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            mapping_data: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, Any]: ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºçµæœã¨ä¿å­˜ã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        """
        logger.info(f"ğŸ—‚ï¸ Starting Categorize stage for session: {session_id}")
        
        await self._update_progress(session_id, "categorize", "processing", 55)
        
        try:
            # ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºå‡¦ç†å®Ÿè¡Œ
            categorized_results = await self.categorize_service.categorize_menu_structure(
                mapping_data, level="paragraph"
            )
            
            logger.info("ğŸ·ï¸ Categorization completed: Menu structure analyzed")
            
            # ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã‚’åŸºæœ¬æƒ…å ±ã§DBä¿å­˜
            saved_entities = await self._save_basic_menu_items(session_id, categorized_results)
            
            # ä¿å­˜ã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            saved_menu_items = [self._entity_to_dict(entity) for entity in saved_entities]
            
            # ğŸ¯ DBæ›´æ–°: ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºçµæœã¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ä¿å­˜
            stage_data = {
                "categories_found": self._extract_categories(categorized_results),
                "menu_items_saved": len(saved_entities),
                "saved_menu_items": saved_menu_items,
                "stage_completed_at": datetime.utcnow().isoformat(),
                "categorization_analysis": {
                    "categories_detected": len(self._extract_categories(categorized_results)),
                    "items_categorized": len(saved_entities),
                    "processing_successful": True
                }
            }
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
            db_update_success = await self._update_session_stage_completion(
                session_id, "categorize_completed", stage_data
            )
            
            # ğŸ¯ SSEé…ä¿¡: Categorizeå®Œäº†é€šçŸ¥ï¼ˆæ±ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            sse_success = await self.redis_publisher.publish_categorize_completion(
                session_id=session_id,
                categorize_results=categorized_results,
                saved_menu_items=saved_menu_items,
                db_saved=db_update_success
            )
            
            if sse_success:
                logger.info(f"ğŸ“¡ Categorize completion broadcasted successfully for session: {session_id}")
            else:
                logger.warning(f"âš ï¸ SSE broadcast failed for categorize completion: {session_id}")
            
            await self._update_progress(session_id, "categorize", "completed", 65)
            
            return {
                "categorized_results": categorized_results,
                "saved_entities": saved_entities,
                "saved_menu_items": saved_menu_items,
                "sse_broadcast_success": sse_success  # SSEé€ä¿¡çµæœã‚’è¿½åŠ 
            }
            
        except Exception as e:
            logger.error(f"âŒ Categorize stage failed for session {session_id}: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®SSEé…ä¿¡
            await self.redis_publisher.publish_error_message(
                session_id=session_id,
                error_type="categorize_processing_failed",
                error_message=str(e),
                task_name="categorize"
            )
            
            raise

    async def _save_basic_menu_items(self, session_id: str, categorized_results: Dict) -> List:
        """åŸºæœ¬ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã‚’DBã«ä¿å­˜"""
        try:
            from app_2.core.database import async_session_factory
            
            async with async_session_factory() as db_session:
                menu_repository = get_menu_repository(db_session)
                menu_save_service = create_menu_save_service(menu_repository)
                
                saved_entities = await menu_save_service.save_categorized_menu_with_session_id(
                    categorized_results, session_id
                )
                
                logger.info(f"ğŸ’¾ Database save completed: {len(saved_entities)} menu items saved")
                return saved_entities
                
        except Exception as e:
            logger.error(f"âŒ Database save failed: {e}")
            return []

    def _entity_to_dict(self, entity) -> Dict[str, Any]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": entity.id,
            "name": entity.name,
            "category": entity.category,
            "category_translation": entity.category_translation,
            "price": entity.price,
            "translation": entity.translation,
            "description": entity.description,
            "allergy": entity.allergy,
            "ingredient": entity.ingredient
        }
        
    async def process_menu_image(
        self, 
        session_id: str, 
        image_data: bytes,
        filename: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç”»åƒã®å®Œå…¨å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆæ®µéšåˆ¥DBæ›´æ–°+SSEé…ä¿¡å¯¾å¿œï¼‰
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            image_data: ç”»åƒãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting enhanced menu processing pipeline: session={session_id}")
            
            # ğŸ”„ é‡è¤‡å®Ÿè¡Œãƒã‚§ãƒƒã‚¯ï¼šåŒã˜ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã§æ—¢ã«å‡¦ç†ä¸­/å®Œäº†ã—ã¦ã„ãªã„ã‹ã‚’ç¢ºèª
            from app_2.core.database import async_session_factory
            async with async_session_factory() as db_session:
                session_repo = get_session_repository(db_session)
                existing_session = await session_repo.get_by_id(session_id)
                
                if existing_session:
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã®è©³ç´°ãƒã‚§ãƒƒã‚¯
                    if existing_session.status == SessionStatus.PROCESSING:
                        logger.warning(f"ğŸ”„ Session {session_id} is already being processed - rejecting duplicate request")
                        return {
                            "status": "error",
                            "session_id": session_id,
                            "error_type": "duplicate_processing",
                            "error_message": f"Session {session_id} is already being processed",
                            "existing_status": existing_session.status.value,
                            "existing_menu_count": len(existing_session.menu_ids or [])
                        }
                    elif existing_session.status == SessionStatus.COMPLETED:
                        logger.warning(f"ğŸ”„ Session {session_id} is already completed - rejecting duplicate request")
                        return {
                            "status": "error", 
                            "session_id": session_id,
                            "error_type": "already_completed",
                            "error_message": f"Session {session_id} is already completed",
                            "existing_status": existing_session.status.value,
                            "existing_menu_count": len(existing_session.menu_ids or [])
                        }
                    else:
                        logger.info(f"ğŸ”„ Session {session_id} exists with status {existing_session.status} - allowing reprocessing")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã¾ãŸã¯æ›´æ–°ï¼ˆUPSERTï¼‰
            async with async_session_factory() as db_session:
                session_repo = get_session_repository(db_session)
                session_entity = SessionEntity(
                    session_id=session_id,
                    status=SessionStatus.PROCESSING,
                    menu_ids=[],
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã¾ãŸã¯æ›´æ–°
                await session_repo.upsert_session(session_entity)
                logger.info(f"âœ… Session entity created/updated with PROCESSING status: {session_id}")
            
            # é–‹å§‹é€šçŸ¥
            await self.redis_publisher.publish_progress_update(
                session_id=session_id,
                task_name="initial_processing",
                status="started",
                progress_data={
                    "phase": "enhanced_pipeline",
                    "filename": filename or "uploaded_image",
                    "stages": ["ocr", "mapping", "categorize", "parallel_tasks"]
                }
            )
            
            # ğŸ”„ Stage 1: OCRå‡¦ç† - DBæ›´æ–°ã¨SSEé…ä¿¡ã‚’å«ã‚€
            ocr_results = await self._execute_ocr_stage(session_id, image_data)
            
            # ğŸ”„ Stage 2: Mappingå‡¦ç† - DBæ›´æ–°ã¨SSEé…ä¿¡ã‚’å«ã‚€
            formatted_mapping_data = await self._execute_mapping_stage(session_id, ocr_results)
            
            # ğŸ”„ Stage 3: Categorizeå‡¦ç† - DBæ›´æ–°ã¨SSEé…ä¿¡ã‚’å«ã‚€
            categorize_data = await self._execute_categorize_stage(session_id, formatted_mapping_data)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ›´æ–°ï¼ˆãƒ¡ãƒ‹ãƒ¥ãƒ¼IDã‚’è¿½åŠ ï¼‰
            saved_entities = categorize_data["saved_entities"]
            if saved_entities:
                menu_ids = [entity.id for entity in saved_entities]
                async with async_session_factory() as db_session:
                    session_repo = get_session_repository(db_session)
                    session_entity = await session_repo.get_by_id(session_id)
                    if session_entity:
                        session_entity.menu_ids = menu_ids
                        session_entity.status = SessionStatus.PROCESSING
                        session_entity.updated_at = datetime.utcnow()
                        await session_repo.update(session_entity)
            
            # Phase 4: ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ãƒˆãƒªã‚¬ãƒ¼ï¼ˆSSEé€ä¿¡æˆåŠŸã‚’æ¡ä»¶ã¨ã™ã‚‹ï¼‰
            sse_broadcast_success = categorize_data.get("sse_broadcast_success", False)
            
            if sse_broadcast_success and saved_entities:
                logger.info(f"Phase 4: Triggering parallel tasks after successful SSE broadcast - session={session_id}")
                await self._update_progress(session_id, "parallel_tasks", "started", 90)
                
                # SSEé€ä¿¡ãŒæˆåŠŸã—ãŸæ™‚ç‚¹ã§DBã¯ç¢ºå®Ÿã«ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ãªã®ã§ã€è¿½åŠ ã®ç¢ºèªã¯ä¸è¦
                logger.info(f"âœ… SSE broadcast confirmed DB commit, triggering parallel tasks with retry-enabled workers")
                await self._trigger_parallel_tasks(session_id, saved_entities)
                
                logger.info(f"ğŸš€ Parallel tasks triggered successfully after SSE confirmation - session={session_id}")
            else:
                if not sse_broadcast_success:
                    logger.warning(f"âš ï¸ Skipping parallel tasks due to SSE broadcast failure - session={session_id}")
                    await self.redis_publisher.publish_error_message(
                        session_id=session_id,
                        error_type="sse_broadcast_failed",
                        error_message="Categorize SSE broadcast failed, parallel tasks not triggered",
                        task_name="parallel_tasks"
                    )
                elif not saved_entities:
                    logger.warning(f"âš ï¸ Skipping parallel tasks due to no saved entities - session={session_id}")
                else:
                    logger.warning(f"âš ï¸ Parallel tasks skipped for unknown reason - session={session_id}")
            
            # åˆæœŸå‡¦ç†å®Œäº†é€šçŸ¥
            await self._update_progress(session_id, "initial_processing", "completed", 100)
            
            # å‡¦ç†æ™‚é–“è¨ˆç®—
            processing_time = time.time() - start_time
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            result = {
                "session_id": session_id,
                "status": "success",
                "filename": filename,
                "file_size": len(image_data),
                "processing_steps": {
                    "step1_ocr": {
                        "description": "Text extraction with position information and realtime broadcast",
                        "text_count": len(ocr_results),
                        "db_updated": True,
                        "sse_broadcasted": True
                    },
                    "step2_mapping": {
                        "description": "Position-based data formatting with realtime broadcast",
                        "formatted_data_length": len(formatted_mapping_data),
                        "db_updated": True,
                        "sse_broadcasted": True
                    },
                    "step3_categorize": {
                        "description": "Menu structure categorization with realtime broadcast",
                        "results": categorize_data["categorized_results"],
                        "saved_items_count": len(saved_entities),
                        "db_updated": True,
                        "sse_broadcasted": True
                    },
                    "step4_parallel_tasks": {
                        "description": "Triggering parallel processing tasks after SSE broadcast confirmation",
                        "sse_broadcast_success": sse_broadcast_success,
                        "parallel_tasks_triggered": sse_broadcast_success and len(saved_entities) > 0,
                        "trigger_condition": "sse_broadcast_confirmed" if sse_broadcast_success else "sse_broadcast_failed"
                    }
                },
                "final_results": categorize_data["categorized_results"],
                "saved_menu_items": categorize_data["saved_menu_items"],
                "categories": self._extract_categories(categorize_data["categorized_results"]),
                "ocr_elements": len(ocr_results),
                "processing_time": round(processing_time, 2),
                "message": f"Enhanced Pipeline: OCR â†’ Mapping â†’ Categorization with realtime DB updates and SSE broadcasts completed successfully. Parallel tasks {'triggered after SSE confirmation' if sse_broadcast_success else 'skipped due to SSE broadcast failure'}."
            }
            
            # ğŸ”„ å‡¦ç†å®Œäº†æ™‚ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’COMPLETEDã«æ›´æ–°
            async with async_session_factory() as db_session:
                session_repo = get_session_repository(db_session)
                session_entity = await session_repo.get_by_id(session_id)
                if session_entity:
                    session_entity.status = SessionStatus.COMPLETED
                    session_entity.updated_at = datetime.utcnow()
                    await session_repo.update(session_entity)
                    logger.info(f"âœ… Session {session_id} marked as COMPLETED")
            
            logger.info(f"Enhanced pipeline processing completed: session={session_id}, time={processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Enhanced pipeline processing failed: {e}", extra={
                "session_id": session_id,
                "image_size": len(image_data) if image_data else 0
            })
            
            # ã‚¨ãƒ©ãƒ¼é€šçŸ¥
            await self.redis_publisher.publish_error_message(
                session_id=session_id,
                error_type="enhanced_pipeline_processing_failed",
                error_message=str(e),
                task_name="enhanced_initial_processing"
            )
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
            try:
                from app_2.core.database import async_session_factory
                async with async_session_factory() as db_session:
                    session_repo = get_session_repository(db_session)
                    session_entity = await session_repo.get_by_id(session_id)
                    if session_entity:
                        session_entity.status = SessionStatus.ERROR
                        session_entity.updated_at = datetime.utcnow()
                        await session_repo.update(session_entity)
            except Exception:
                pass
            
            raise
    
    async def _update_progress(
        self, 
        session_id: str, 
        task_name: str, 
        status: str, 
        progress: int
    ):
        """é€²æ—æ›´æ–°ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰"""
        await self.redis_publisher.publish_progress_update(
            session_id=session_id,
            task_name=task_name,
            status=status,
            progress_data={"progress": progress}
        )
    
    def _extract_categories(self, categorized_result: Dict[str, Any]) -> List[str]:
        """ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºçµæœã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåã‚’æŠ½å‡º"""
        try:
            if "menu" in categorized_result and "categories" in categorized_result["menu"]:
                return [
                    cat.get("name", "unknown") 
                    for cat in categorized_result["menu"]["categories"]
                ]
            return []
        except Exception:
            return []
    
    async def _trigger_parallel_tasks(self, session_id: str, menu_entities: List):
        """ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®ãƒˆãƒªã‚¬ãƒ¼ï¼ˆç¿»è¨³ + è©³ç´°èª¬æ˜ + ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ + å†…å®¹ç‰©ã‚’åŒæ™‚å®Ÿè¡Œï¼‰"""
        try:
            # ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            menu_items_data = [
                {
                    "id": entity.id,
                    "name": entity.name,
                    "category": entity.category,
                    "price": entity.price,
                    "translation": entity.translation,
                    "category_translation": entity.category_translation
                }
                for entity in menu_entities
            ]
            
            logger.info(f"Triggering parallel tasks with {len(menu_items_data)} menu items")
            
            # ğŸ¯ ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚å®Ÿè¡Œï¼šç¿»è¨³ + è©³ç´°èª¬æ˜ + ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ + å†…å®¹ç‰© + ç”»åƒæ¤œç´¢
            
            # ç¿»è¨³ã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼
            from app_2.tasks.translate_task import translate_menu_task
            translate_task_result = translate_menu_task.delay(session_id, menu_items_data)
            
            # è©³ç´°èª¬æ˜ã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼ï¼ˆåŒæ™‚å®Ÿè¡Œï¼‰
            from app_2.tasks.describe_task import describe_menu_task
            describe_task_result = describe_menu_task.delay(session_id, menu_items_data)
            
            # ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼è§£æã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼ï¼ˆåŒæ™‚å®Ÿè¡Œï¼‰
            from app_2.tasks.allergen_task import allergen_menu_task
            allergen_task_result = allergen_menu_task.delay(session_id, menu_items_data)
            
            # å†…å®¹ç‰©è§£æã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼ï¼ˆåŒæ™‚å®Ÿè¡Œï¼‰
            from app_2.tasks.ingredient_task import ingredient_menu_task
            ingredient_task_result = ingredient_menu_task.delay(session_id, menu_items_data)
            
            # ç”»åƒæ¤œç´¢ã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼ï¼ˆåŒæ™‚å®Ÿè¡Œï¼‰
            from app_2.tasks.search_image_task import search_image_menu_task
            search_image_task_result = search_image_menu_task.delay(session_id, menu_items_data)
            
            logger.info(f"âœ… Translation task triggered: task_id={translate_task_result.id}")
            logger.info(f"âœ… Description task triggered: task_id={describe_task_result.id}")
            logger.info(f"âœ… Allergen task triggered: task_id={allergen_task_result.id}")
            logger.info(f"âœ… Ingredient task triggered: task_id={ingredient_task_result.id}")
            logger.info(f"âœ… Search Image task triggered: task_id={search_image_task_result.id}")
            
            # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯é–‹å§‹ã®è©³ç´°é€šçŸ¥
            await self.redis_publisher.publish_session_message(
                session_id=session_id,
                message_type="parallel_tasks_started",
                data={
                    "parallel_tasks": ["translation", "description", "allergen", "ingredient", "search_image"],
                    "task_ids": {
                        "translation": translate_task_result.id,
                        "description": describe_task_result.id,
                        "allergen": allergen_task_result.id,
                        "ingredient": ingredient_task_result.id,
                        "search_image": search_image_task_result.id
                    },
                    "total_items": len(menu_items_data),
                    "execution_mode": "parallel",
                    "message": f"Translation, description, allergen analysis, ingredient analysis, and image search started in parallel for {len(menu_items_data)} items"
                }
            )
            
            logger.info(f"ğŸš€ All parallel tasks (translation + description + allergen + ingredient + search_image) triggered for session: {session_id}")
            
        except Exception as e:
            logger.error(f"Failed to trigger parallel tasks: {e}")
            
            await self.redis_publisher.publish_error_message(
                session_id=session_id,
                error_type="parallel_tasks_trigger_failed", 
                error_message=str(e),
                task_name="parallel_tasks"
            )


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_pipeline_instance = None

def get_menu_processing_pipeline() -> MenuProcessingPipeline:
    """MenuProcessingPipelineã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = MenuProcessingPipeline()
    return _pipeline_instance 