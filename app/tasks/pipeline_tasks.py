#!/usr/bin/env python3
"""
å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ– Celeryã‚¿ã‚¹ã‚¯

Stage 1-5å…¨ä½“ã®å®Œå…¨ä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ç¾¤
- å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆStage 1â†’2â†’3â†’4â†’5ã®æœ€é©åŒ–ã•ã‚ŒãŸé€£ç¶šå®Ÿè¡Œï¼‰
- ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆã‚«ãƒ†ã‚´ãƒªã”ã¨ã®Stage 3â†’4â†’5ä¸¦åˆ—å®Ÿè¡Œï¼‰
- Stageé–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†ï¼ˆStage 3å®Œäº†â†’å³åº§Stage 5é–‹å§‹ï¼‰
- æ®µéšçš„çµæœã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆå„Stageå®Œäº†ã”ã¨ã®éƒ¨åˆ†çµæœé…ä¿¡ï¼‰
- è¤‡æ•°ç”»åƒä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè¤‡æ•°ç”»åƒã®åŒæ™‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼‰
"""

import time
import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from .celery_app import celery_app

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)

def await_sync(coro):
    """éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œï¼ˆCeleryãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ï¼‰"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(coro)
    finally:
        loop.close()

@celery_app.task(bind=True, name="full_pipeline_process")
def full_pipeline_process(self, image_path: str, session_id: str = None, options: Dict = None):
    """
    å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆStage 1-5å…¨ä½“æœ€é©åŒ–ï¼‰
    
    Args:
        image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        options: å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
    Returns:
        Dict: å…¨Stageå®Œäº†çµæœ
    """
    try:
        start_time = time.time()
        options = options or {}
        
        # åˆæœŸé€²è¡ŒçŠ¶æ³æ›´æ–°
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_full',
                'progress': 0,
                'status': 'initializing',
                'pipeline_mode': 'full_parallel_optimization',
                'current_stage': 1,
                'total_stages': 5
            }
        )
        
        logger.info(f"ğŸš€ Starting FULL PIPELINE processing for: {image_path}")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœã‚’æ ¼ç´
        pipeline_results = {}
        pipeline_metadata = {
            'pipeline_enabled': True,
            'pipeline_mode': 'full_parallel_optimization',
            'start_time': start_time,
            'stage_timing': {},
            'optimizations_applied': []
        }
        
        # === Stage 1: OCR (ä¸¦åˆ—å‡¦ç†ç‰ˆ) ===
        stage1_start = time.time()
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_full',
                'progress': 5,
                'status': 'stage1_ocr',
                'current_stage': 1,
                'stage_name': 'OCR (Parallel Multi-Engine)'
            }
        )
        
        from app.workflows.stages import stage1_ocr_gemini_exclusive
        stage1_result = await_sync(stage1_ocr_gemini_exclusive(image_path, session_id))
        
        if not stage1_result["success"]:
            raise Exception(f"Stage 1 OCR failed: {stage1_result.get('error', 'Unknown error')}")
        
        pipeline_results['stage1'] = stage1_result
        pipeline_metadata['stage_timing']['stage1'] = time.time() - stage1_start
        pipeline_metadata['optimizations_applied'].append('parallel_ocr')
        
        extracted_text = stage1_result["extracted_text"]
        if not extracted_text.strip():
            return {
                'success': False,
                'error': 'No text extracted from image',
                'pipeline_results': pipeline_results,
                'pipeline_metadata': pipeline_metadata
            }
        
        # === Stage 2: ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º ===
        stage2_start = time.time()
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_full',
                'progress': 25,
                'status': 'stage2_categorize',
                'current_stage': 2,
                'stage_name': 'Categorization (OpenAI Function Calling)'
            }
        )
        
        from app.workflows.stages import stage2_categorize_openai_exclusive
        stage2_result = await_sync(stage2_categorize_openai_exclusive(extracted_text, session_id))
        
        if not stage2_result["success"]:
            raise Exception(f"Stage 2 Categorization failed: {stage2_result.get('error', 'Unknown error')}")
        
        pipeline_results['stage2'] = stage2_result
        pipeline_metadata['stage_timing']['stage2'] = time.time() - stage2_start
        
        categorized_data = stage2_result["categories"]
        
        # ã‚«ãƒ†ã‚´ãƒªæ•°ã¨ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’ç¢ºèªã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æˆ¦ç•¥ã‚’æ±ºå®š
        total_categories = len(categorized_data)
        total_items = sum(len(items) for items in categorized_data.values())
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æˆ¦ç•¥åˆ¤å®š
        from app.core.config import settings
        use_category_pipeline = (
            settings.ENABLE_CATEGORY_PIPELINING and 
            total_categories >= settings.MIN_CATEGORIES_FOR_OVERLAP and
            total_items >= settings.PIPELINE_ITEM_THRESHOLD
        )
        
        use_stage3_to_stage5_overlap = (
            settings.STAGE3_TO_STAGE5_OVERLAP and
            total_items >= 5
        )
        
        if use_category_pipeline:
            pipeline_metadata['optimizations_applied'].append('category_level_pipelining')
            logger.info(f"ğŸ“‹ Using CATEGORY LEVEL PIPELINING: {total_categories} categories, {total_items} items")
            
            # ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œ
            pipeline_stage345_result = _execute_category_level_pipeline(
                self, categorized_data, session_id, pipeline_metadata
            )
            
        else:
            pipeline_metadata['optimizations_applied'].append('sequential_stage_processing')
            logger.info(f"ğŸ“‹ Using SEQUENTIAL PROCESSING: {total_categories} categories, {total_items} items")
            
            # é€æ¬¡Stageå®Ÿè¡Œ
            pipeline_stage345_result = _execute_sequential_stages(
                self, categorized_data, session_id, pipeline_metadata, use_stage3_to_stage5_overlap
            )
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‚’çµ±åˆ
        pipeline_results.update(pipeline_stage345_result['results'])
        pipeline_metadata.update(pipeline_stage345_result['metadata'])
        
        # æœ€çµ‚çµæœæ§‹ç¯‰
        total_time = time.time() - start_time
        pipeline_metadata['total_processing_time'] = total_time
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_full',
                'progress': 100,
                'status': 'completed',
                'total_processing_time': total_time,
                'optimizations_applied': pipeline_metadata['optimizations_applied']
            }
        )
        
        final_result = {
            'success': True,
            'pipeline_processing': True,
            'pipeline_mode': 'full_parallel_optimization',
            'total_processing_time': total_time,
            'total_categories': total_categories,
            'total_items': total_items,
            'optimizations_applied': pipeline_metadata['optimizations_applied'],
            
            # å„Stageçµæœ
            'extracted_text': extracted_text,
            'categories': categorized_data,
            'translated_categories': pipeline_results.get('stage3', {}).get('translated_categories', {}),
            'final_menu': pipeline_results.get('stage4', {}).get('final_menu', {}),
            'images_generated': pipeline_results.get('stage5', {}).get('images_generated', {}),
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'pipeline_metadata': pipeline_metadata,
            'stage_timing': pipeline_metadata['stage_timing']
        }
        
        logger.info(f"ğŸ‰ FULL PIPELINE completed in {total_time:.2f}s: {total_items} items, {len(pipeline_metadata['optimizations_applied'])} optimizations")
        
        return final_result
        
    except Exception as e:
        logger.error(f"Full pipeline processing failed: {str(e)}")
        
        self.update_state(
            state='FAILURE',
            meta={
                'stage': 'pipeline_full',
                'error': str(e),
                'status': 'failed',
                'exc_type': type(e).__name__,
                'exc_module': type(e).__module__
            }
        )
        
        return {
            'success': False,
            'error': f"Full pipeline error: {str(e)}",
            'pipeline_processing': True,
            'pipeline_mode': 'full_parallel_optimization_failed'
        }

def _execute_category_level_pipeline(task_self, categorized_data: Dict, session_id: str, pipeline_metadata: Dict) -> Dict:
    """ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œ"""
    try:
        start_time = time.time()
        
        # === Stage 3: ä¸¦åˆ—ç¿»è¨³ ===
        stage3_start = time.time()
        task_self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_category',
                'progress': 35,
                'status': 'stage3_parallel_translation',
                'current_stage': 3,
                'stage_name': 'Parallel Translation'
            }
        )
        
        from app.workflows.stages import stage3_translate_with_fallback
        stage3_result = await_sync(stage3_translate_with_fallback(categorized_data, session_id))
        
        if not stage3_result["success"]:
            raise Exception(f"Stage 3 Translation failed: {stage3_result.get('error', 'Unknown error')}")
        
        pipeline_metadata['stage_timing']['stage3'] = time.time() - stage3_start
        translated_categories = stage3_result["translated_categories"]
        
        # === ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Stage 4 & 5 ===
        category_pipeline_tasks = []
        
        for category_name, items in translated_categories.items():
            if items:  # ç©ºã§ãªã„ã‚«ãƒ†ã‚´ãƒªã®ã¿
                # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«Stage 4â†’5ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                task = category_stage45_pipeline.delay(
                    category_name, {category_name: items}, session_id
                )
                category_pipeline_tasks.append((category_name, task))
        
        task_self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_category',
                'progress': 55,
                'status': 'stage45_category_pipeline',
                'pipeline_tasks': len(category_pipeline_tasks),
                'stage_name': 'Category Level Pipeline (Stage 4+5)'
            }
        )
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‚’åé›†
        final_menu = {}
        images_generated = {}
        category_failures = []
        
        for category_name, task in category_pipeline_tasks:
            try:
                result = task.get(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result['success']:
                    final_menu.update(result['final_menu'])
                    images_generated.update(result['images_generated'])
                    logger.info(f"âœ… Category pipeline completed: {category_name}")
                else:
                    category_failures.append({
                        'category': category_name,
                        'error': result.get('error', 'Unknown error')
                    })
                    logger.warning(f"âš ï¸ Category pipeline failed: {category_name}")
                    
            except Exception as e:
                category_failures.append({
                    'category': category_name,
                    'error': f"Task execution failed: {str(e)}"
                })
                logger.error(f"âŒ Category pipeline task failed: {category_name}: {str(e)}")
        
        pipeline_metadata['stage_timing']['stage45_pipeline'] = time.time() - start_time
        pipeline_metadata['category_failures'] = category_failures
        
        return {
            'results': {
                'stage3': stage3_result,
                'stage4': {
                    'success': len(category_failures) == 0,
                    'final_menu': final_menu,
                    'category_failures': category_failures
                },
                'stage5': {
                    'success': len(images_generated) > 0,
                    'images_generated': images_generated
                }
            },
            'metadata': pipeline_metadata
        }
        
    except Exception as e:
        logger.error(f"Category level pipeline failed: {str(e)}")
        raise

def _execute_sequential_stages(task_self, categorized_data: Dict, session_id: str, pipeline_metadata: Dict, use_overlap: bool) -> Dict:
    """é€æ¬¡Stageå‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦Stage 3â†’5ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼‰"""
    try:
        # === Stage 3: ä¸¦åˆ—ç¿»è¨³ ===
        stage3_start = time.time()
        task_self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_sequential',
                'progress': 35,
                'status': 'stage3_parallel_translation',
                'current_stage': 3
            }
        )
        
        from app.workflows.stages import stage3_translate_with_fallback
        stage3_result = await_sync(stage3_translate_with_fallback(categorized_data, session_id))
        
        if not stage3_result["success"]:
            raise Exception(f"Stage 3 Translation failed: {stage3_result.get('error', 'Unknown error')}")
        
        pipeline_metadata['stage_timing']['stage3'] = time.time() - stage3_start
        translated_categories = stage3_result["translated_categories"]
        
        # Stage 3å®Œäº†â†’å³åº§ã«Stage 5é–‹å§‹ã®å‡¦ç†ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼‰
        stage5_task = None
        if use_overlap:
            pipeline_metadata['optimizations_applied'].append('stage3_to_stage5_overlap')
            
            try:
                # Stage 5ã‚’éåŒæœŸã§é–‹å§‹
                from app.workflows.stages import stage5_generate_images
                stage5_future = asyncio.create_task(stage5_generate_images(translated_categories, session_id))
                logger.info("ğŸ”„ Stage 5 started in parallel with Stage 4")
                
            except Exception as e:
                logger.warning(f"Failed to start Stage 5 in parallel: {str(e)}")
                stage5_future = None
        
        # === Stage 4: ä¸¦åˆ—è©³ç´°èª¬æ˜ ===
        stage4_start = time.time()
        task_self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_sequential',
                'progress': 65,
                'status': 'stage4_parallel_description',
                'current_stage': 4
            }
        )
        
        from app.workflows.stages import stage4_add_descriptions
        stage4_result = await_sync(stage4_add_descriptions(translated_categories, session_id))
        
        if not stage4_result["success"]:
            raise Exception(f"Stage 4 Description failed: {stage4_result.get('error', 'Unknown error')}")
        
        pipeline_metadata['stage_timing']['stage4'] = time.time() - stage4_start
        final_menu = stage4_result["final_menu"]
        
        # === Stage 5: ç”»åƒç”Ÿæˆ ===
        stage5_start = time.time()
        task_self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'pipeline_sequential',
                'progress': 85,
                'status': 'stage5_images',
                'current_stage': 5
            }
        )
        
        if use_overlap and 'stage5_future' in locals() and stage5_future:
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—Stage 5ã®çµæœã‚’å–å¾—
            try:
                stage5_result = await_sync(stage5_future)
                logger.info("âœ… Overlapped Stage 5 completed")
                
            except Exception as e:
                logger.warning(f"Overlapped Stage 5 failed, falling back to sequential: {str(e)}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé€šå¸¸ã®Stage 5å®Ÿè¡Œ
                from app.workflows.stages import stage5_generate_images
                stage5_result = await_sync(stage5_generate_images(final_menu, session_id))
        
        else:
            # é€šå¸¸ã®Stage 5å®Ÿè¡Œ
            from app.workflows.stages import stage5_generate_images
            stage5_result = await_sync(stage5_generate_images(final_menu, session_id))
        
        pipeline_metadata['stage_timing']['stage5'] = time.time() - stage5_start
        
        return {
            'results': {
                'stage3': stage3_result,
                'stage4': stage4_result,
                'stage5': stage5_result
            },
            'metadata': pipeline_metadata
        }
        
    except Exception as e:
        logger.error(f"Sequential stages execution failed: {str(e)}")
        raise

@celery_app.task(bind=True, name="category_stage45_pipeline")
def category_stage45_pipeline(self, category_name: str, category_data: Dict, session_id: str = None):
    """
    å˜ä¸€ã‚«ãƒ†ã‚´ãƒªã®Stage 4â†’5ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼
    
    Args:
        category_name: ã‚«ãƒ†ã‚´ãƒªå
        category_data: ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ {category_name: items}
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        
    Returns:
        Dict: Stage 4+5å®Œäº†çµæœ
    """
    try:
        start_time = time.time()
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'category_pipeline',
                'category': category_name,
                'progress': 0,
                'status': 'starting_stage4'
            }
        )
        
        logger.info(f"ğŸ”„ Starting Category Pipeline for: {category_name}")
        
        # === Stage 4: ã‚«ãƒ†ã‚´ãƒªè©³ç´°èª¬æ˜ ===
        from app.workflows.stages import stage4_add_descriptions
        stage4_result = await_sync(stage4_add_descriptions(category_data, session_id))
        
        if not stage4_result["success"]:
            raise Exception(f"Stage 4 failed for category {category_name}: {stage4_result.get('error', 'Unknown error')}")
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'category_pipeline',
                'category': category_name,
                'progress': 50,
                'status': 'starting_stage5'
            }
        )
        
        final_menu = stage4_result["final_menu"]
        
        # === Stage 5: ã‚«ãƒ†ã‚´ãƒªç”»åƒç”Ÿæˆ ===
        from app.workflows.stages import stage5_generate_images
        stage5_result = await_sync(stage5_generate_images(final_menu, session_id))
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'category_pipeline',
                'category': category_name,
                'progress': 100,
                'status': 'completed'
            }
        )
        
        total_time = time.time() - start_time
        
        result = {
            'success': True,
            'category': category_name,
            'final_menu': final_menu,
            'images_generated': stage5_result.get('images_generated', {}),
            'processing_time': total_time,
            'pipeline_mode': 'category_stage45_pipeline'
        }
        
        logger.info(f"âœ… Category Pipeline completed: {category_name} in {total_time:.2f}s")
        
        return result
        
    except Exception as e:
        logger.error(f"Category pipeline failed for {category_name}: {str(e)}")
        
        self.update_state(
            state='FAILURE',
            meta={
                'stage': 'category_pipeline',
                'category': category_name,
                'error': str(e),
                'status': 'failed',
                'exc_type': type(e).__name__,
                'exc_module': type(e).__module__
            }
        )
        
        return {
            'success': False,
            'category': category_name,
            'error': f"Category pipeline error: {str(e)}",
            'pipeline_mode': 'category_stage45_pipeline_failed'
        }

@celery_app.task(bind=True, name="multiple_images_pipeline")
def multiple_images_pipeline(self, image_paths: List[str], session_id: str = None, options: Dict = None):
    """
    è¤‡æ•°ç”»åƒä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼
    
    Args:
        image_paths: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        options: å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
    Returns:
        Dict: å…¨ç”»åƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†çµæœ
    """
    try:
        start_time = time.time()
        options = options or {}
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'multi_image_pipeline',
                'progress': 0,
                'status': 'initializing',
                'total_images': len(image_paths),
                'pipeline_mode': 'multiple_images_parallel'
            }
        )
        
        logger.info(f"ğŸš€ Starting MULTIPLE IMAGES PIPELINE: {len(image_paths)} images")
        
        # ç”»åƒã”ã¨ã«ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        pipeline_tasks = []
        
        for i, image_path in enumerate(image_paths):
            task = full_pipeline_process.delay(image_path, f"{session_id}_img{i}", options)
            pipeline_tasks.append((f"image_{i}", image_path, task))
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'multi_image_pipeline',
                'progress': 10,
                'status': 'parallel_processing',
                'tasks_created': len(pipeline_tasks)
            }
        )
        
        # å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‚’åé›†
        all_results = {}
        failed_images = []
        
        for image_name, image_path, task in pipeline_tasks:
            try:
                result = task.get(timeout=1200)  # 20åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result['success']:
                    all_results[image_name] = result
                    logger.info(f"âœ… Image pipeline completed: {image_name}")
                else:
                    failed_images.append({
                        'image': image_name,
                        'path': image_path,
                        'error': result.get('error', 'Unknown error')
                    })
                    logger.warning(f"âš ï¸ Image pipeline failed: {image_name}")
                    
            except Exception as e:
                failed_images.append({
                    'image': image_name,
                    'path': image_path,
                    'error': f"Task execution failed: {str(e)}"
                })
                logger.error(f"âŒ Image pipeline task failed: {image_name}: {str(e)}")
        
        total_time = time.time() - start_time
        
        self.update_state(
            state='PROGRESS',
            meta={
                'stage': 'multi_image_pipeline',
                'progress': 100,
                'status': 'completed',
                'successful_images': len(all_results),
                'failed_images': len(failed_images)
            }
        )
        
        result = {
            'success': len(failed_images) == 0,
            'pipeline_processing': True,
            'pipeline_mode': 'multiple_images_parallel',
            'total_processing_time': total_time,
            'total_images': len(image_paths),
            'successful_images': len(all_results),
            'failed_images': len(failed_images),
            'all_results': all_results,
            'failed_images': failed_images if failed_images else None
        }
        
        logger.info(f"ğŸ‰ MULTIPLE IMAGES PIPELINE completed in {total_time:.2f}s: {len(all_results)}/{len(image_paths)} images")
        
        return result
        
    except Exception as e:
        logger.error(f"Multiple images pipeline failed: {str(e)}")
        
        self.update_state(
            state='FAILURE',
            meta={
                'stage': 'multi_image_pipeline',
                'error': str(e),
                'status': 'failed',
                'exc_type': type(e).__name__,
                'exc_module': type(e).__module__
            }
        )
        
        return {
            'success': False,
            'error': f"Multiple images pipeline error: {str(e)}",
            'pipeline_processing': True,
            'pipeline_mode': 'multiple_images_parallel_failed'
        } 