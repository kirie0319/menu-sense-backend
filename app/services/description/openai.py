import json
import asyncio
from typing import Optional
from concurrent.futures import ThreadPoolExecutor

try:
    import openai
    from openai import AsyncOpenAI
except ImportError:
    openai = None
    AsyncOpenAI = None

from app.core.config import settings
from .base import BaseDescriptionService, DescriptionResult, DescriptionProvider

class OpenAIDescriptionService(BaseDescriptionService):
    """OpenAI APIã‚’ä½¿ç”¨ã—ãŸè©³ç´°èª¬æ˜ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        super().__init__()
        self.provider = DescriptionProvider.OPENAI
        self.client = None
        self._initialize_client()
        # ä¸¦åˆ—å‡¦ç†ç”¨ã‚»ãƒãƒ•ã‚©ï¼ˆåŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼‰
        self.semaphore = asyncio.Semaphore(settings.CONCURRENT_CHUNK_LIMIT)
    
    def _initialize_client(self):
        """OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            if not openai or not AsyncOpenAI:
                print("âŒ openai package not installed. Install with: pip install openai")
                return
                
            if settings.OPENAI_API_KEY:
                self.client = AsyncOpenAI(
                    api_key=settings.OPENAI_API_KEY,
                    timeout=settings.OPENAI_TIMEOUT,
                    max_retries=settings.OPENAI_MAX_RETRIES
                )
                print("ğŸ”§ OpenAI Description Service initialized successfully")
            else:
                print("âš ï¸ OPENAI_API_KEY not set")
                
        except Exception as e:
            print(f"âŒ Failed to initialize OpenAI Description Service: {e}")
            self.client = None
    
    def is_available(self) -> bool:
        """ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.client is not None and bool(settings.OPENAI_API_KEY)
    
    async def call_openai_with_retry(self, messages, max_retries=2):
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ãã§OpenAI APIã‚’å‘¼ã³å‡ºã™ãƒªãƒˆãƒ©ã‚¤é–¢æ•°ï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ç”¨ï¼‰"""
        if not self.is_available():
            raise Exception("OpenAI API is not available")
        
        for attempt in range(max_retries + 1):
            try:
                response = await self.client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=messages
                )
                return response
                
            except openai.RateLimitError as e:
                if attempt == max_retries:
                    raise Exception(f"Rate limit exceeded after {max_retries + 1} attempts: {str(e)}")
                
                wait_time = settings.RETRY_BASE_DELAY ** attempt
                print(f"â³ Rate limit hit, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                await asyncio.sleep(wait_time)
                
            except openai.APITimeoutError as e:
                if attempt == max_retries:
                    raise Exception(f"API timeout after {max_retries + 1} attempts: {str(e)}")
                
                wait_time = settings.RETRY_BASE_DELAY ** attempt
                print(f"â³ API timeout, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                await asyncio.sleep(wait_time)
                
            except openai.APIConnectionError as e:
                if attempt == max_retries:
                    raise Exception(f"API connection error after {max_retries + 1} attempts: {str(e)}")
                
                wait_time = settings.RETRY_BASE_DELAY ** attempt
                print(f"â³ Connection error, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å¤±æ•—
                raise Exception(f"OpenAI API error: {str(e)}")
    
    def create_description_prompt(self, category: str, chunk: list) -> str:
        """è©³ç´°èª¬æ˜ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        example_descriptions = self.get_example_descriptions()
        examples_text = "\n".join([
            f"- \"{dish}\" â†’ \"{desc}\""
            for dish, desc in list(example_descriptions.items())[:3]
        ])
        
        return f"""ä»¥ä¸‹ã®ç¿»è¨³æ¸ˆã¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼é …ç›®ã«ã€å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã®è©³ç´°èª¬æ˜ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

ã‚«ãƒ†ã‚´ãƒª: {category}
é …ç›®æ•°: {len(chunk)}

ãƒ‡ãƒ¼ã‚¿:
{json.dumps({category: chunk}, ensure_ascii=False, indent=2)}

è¦ä»¶:
1. å„æ–™ç†ã«è©³ç´°ãªè‹±èªèª¬æ˜ã‚’è¿½åŠ 
2. èª¿ç†æ³•ã€ä½¿ç”¨é£Ÿæã€å‘³ã®ç‰¹å¾´ã‚’å«ã‚ã‚‹  
3. å¤–å›½äººãŒç†è§£ã—ã‚„ã™ã„è¡¨ç¾ã‚’ä½¿ç”¨
4. æ–‡åŒ–çš„èƒŒæ™¯ã‚‚ç°¡æ½”ã«èª¬æ˜
5. è¦³å…‰å®¢å‘ã‘ã«é­…åŠ›çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã«ã™ã‚‹

å¿…ãšJSONå½¢å¼ã§è¿”ç­”ã—ã¦ãã ã•ã„:
{{
  "final_menu": {{
    "{category}": [
      {{
        "japanese_name": "æ—¥æœ¬èªå",
        "english_name": "è‹±èªå", 
        "description": "è©³ç´°ãªè‹±èªèª¬æ˜",
        "price": "ä¾¡æ ¼ï¼ˆã‚ã‚Œã°ï¼‰"
      }}
    ]
  }}
}}

ä¾‹ã®èª¬æ˜:
{examples_text}

å„èª¬æ˜ã¯30-80èªç¨‹åº¦ã§ã€æ–™ç†ã®ç‰¹å¾´ã¨é­…åŠ›ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚"""
    
    def extract_json_from_response(self, content: str) -> dict:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º"""
        # JSONã®é–‹å§‹ã¨çµ‚äº†ã‚’æ¢ã™
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start == -1 or json_end == -1:
            raise ValueError("No JSON found in response")
        
        json_str = content[json_start:json_end]
        return json.loads(json_str)
    
    async def process_chunk(
        self, 
        category: str, 
        chunk: list, 
        chunk_number: int, 
        total_chunks: int,
        session_id: Optional[str] = None
    ) -> list:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã—ã¦è©³ç´°èª¬æ˜ã‚’è¿½åŠ """
        print(f"  ğŸ“¦ Processing chunk {chunk_number}/{total_chunks} ({len(chunk)} items)")
        
        # é€²è¡ŒçŠ¶æ³é€šçŸ¥ï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ä¸­ï¼‰
        if session_id:
            from app.services.realtime import send_progress
            await send_progress(
                session_id, 4, "active", 
                f"ğŸ”„ Processing {category} (part {chunk_number}/{total_chunks})",
                {"chunk_progress": f"{chunk_number}/{total_chunks}"}
            )
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            messages = [
                {
                    "role": "user",
                    "content": self.create_description_prompt(category, chunk)
                }
            ]
            
            # OpenAI APIå‘¼ã³å‡ºã—
            response = await self.call_openai_with_retry(messages, max_retries=2)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º
            content = response.choices[0].message.content
            chunk_result = self.extract_json_from_response(content)
            
            if 'final_menu' in chunk_result and category in chunk_result['final_menu']:
                new_items = chunk_result['final_menu'][category]
                
                # èª¬æ˜ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                for item in new_items:
                    if item.get("description"):
                        item["description"] = self.clean_description_text(item["description"])
                
                print(f"    âœ… Successfully processed {len(new_items)} items in chunk")
                return new_items
            else:
                raise ValueError("Invalid response format")
                
        except Exception as chunk_error:
            print(f"âš ï¸ Chunk processing error: {chunk_error}")
            print(f"    ğŸ”„ Using fallback descriptions for chunk {chunk_number}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èª¬æ˜ã‚’ç”Ÿæˆ
            fallback_items = []
            for item in chunk:
                fallback_description = self.create_fallback_description(item)
                fallback_items.append({
                    "japanese_name": item.get("japanese_name", "N/A"),
                    "english_name": item.get("english_name", "N/A"),
                    "description": fallback_description,
                    "price": item.get("price", "")
                })
            
            return fallback_items
    
    async def process_chunk_with_semaphore(
        self, 
        category: str, 
        chunk: list, 
        chunk_number: int, 
        total_chunks: int,
        session_id: Optional[str] = None
    ) -> tuple:
        """ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†"""
        async with self.semaphore:
            print(f"  ğŸš€ Starting parallel chunk {chunk_number}/{total_chunks} ({len(chunk)} items)")
            
            # é€²è¡ŒçŠ¶æ³é€šçŸ¥ï¼ˆãƒãƒ£ãƒ³ã‚¯é–‹å§‹ï¼‰
            if session_id:
                from app.services.realtime import send_progress
                await send_progress(
                    session_id, 4, "active", 
                    f"ğŸ”„ Starting parallel processing for {category} (chunk {chunk_number}/{total_chunks})",
                    {
                        "chunk_progress": f"{chunk_number}/{total_chunks}",
                        "parallel_processing": True,
                        "chunk_started": chunk_number
                    }
                )
            
            try:
                # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ
                result = await self.process_chunk(category, chunk, chunk_number, total_chunks, session_id)
                
                print(f"  âœ… Completed parallel chunk {chunk_number}/{total_chunks}")
                return (chunk_number, result, None)  # (chunk_number, result, error)
                
            except Exception as e:
                print(f"  âŒ Error in parallel chunk {chunk_number}/{total_chunks}: {e}")
                return (chunk_number, None, str(e))

    async def process_category_parallel(
        self,
        category: str,
        items: list,
        session_id: Optional[str] = None
    ) -> list:
        """ã‚«ãƒ†ã‚´ãƒªå†…ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        if not items:
            return []
            
        print(f"ğŸ”„ Processing category: {category} ({len(items)} items) - PARALLEL MODE")
        
        # é€²è¡ŒçŠ¶æ³é€šçŸ¥ï¼ˆã‚«ãƒ†ã‚´ãƒªé–‹å§‹ï¼‰
        if session_id:
            from app.services.realtime import send_progress
            await send_progress(
                session_id, 4, "active", 
                f"ğŸ½ï¸ Adding descriptions for {category} (parallel processing)...",
                {
                    "processing_category": category,
                    "parallel_mode": True,
                    "total_items": len(items)
                }
            )
        
        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunk_size = settings.PROCESSING_CHUNK_SIZE
        chunks = []
        
        for i in range(0, len(items), chunk_size):
            chunk = items[i:i + chunk_size]
            chunk_number = (i // chunk_size) + 1
            total_chunks = (len(items) + chunk_size - 1) // chunk_size
            chunks.append((chunk, chunk_number, total_chunks))
        
        print(f"  ğŸ“¦ Created {len(chunks)} chunks for parallel processing")
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—ã§å‡¦ç†
        tasks = []
        for chunk, chunk_number, total_chunks in chunks:
            task = self.process_chunk_with_semaphore(
                category, chunk, chunk_number, total_chunks, session_id
            )
            tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹
        print(f"  ğŸš€ Starting {len(tasks)} parallel chunk tasks...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’å‡¦ç†
        category_results = []
        successful_chunks = 0
        failed_chunks = 0
        
        # ãƒãƒ£ãƒ³ã‚¯ç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆå…ƒã®é †åºã‚’ç¶­æŒï¼‰
        sorted_results = []
        for result in results:
            if isinstance(result, tuple):
                sorted_results.append(result)
            else:
                # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆ
                print(f"  âš ï¸ Exception in parallel processing: {result}")
                failed_chunks += 1
        
        sorted_results.sort(key=lambda x: x[0])  # chunk_numberã§ã‚½ãƒ¼ãƒˆ
        
        for chunk_number, chunk_result, error in sorted_results:
            if error:
                print(f"  âš ï¸ Chunk {chunk_number} failed: {error}")
                failed_chunks += 1
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆå¯¾å¿œã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰
                # ã“ã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ãŒå¿…è¦ï¼‰
            elif chunk_result:
                category_results.extend(chunk_result)
                successful_chunks += 1
                
                # ãƒãƒ£ãƒ³ã‚¯å®Œäº†ã®é€²æ—é€ä¿¡
                if session_id:
                    await send_progress(
                        session_id, 4, "active", 
                        f"ğŸ½ï¸ {category}: Chunk {chunk_number} completed ({len(chunk_result)} items)",
                        {
                            "processing_category": category,
                            "chunk_completed": chunk_number,
                            "chunk_result": chunk_result,
                            "parallel_processing": True,
                            "successful_chunks": successful_chunks,
                            "failed_chunks": failed_chunks
                        }
                    )
        
        print(f"  âœ… Parallel processing complete: {successful_chunks} successful, {failed_chunks} failed chunks")
        
        # ã‚«ãƒ†ã‚´ãƒªå®Œäº†é€šçŸ¥
        if session_id:
            await send_progress(
                session_id, 4, "active", 
                f"âœ… {category}ã‚«ãƒ†ã‚´ãƒªå®Œäº†ï¼{len(category_results)}ã‚¢ã‚¤ãƒ†ãƒ ã®è©³ç´°èª¬æ˜ã‚’ä¸¦åˆ—å‡¦ç†ã§è¿½åŠ ã—ã¾ã—ãŸ",
                {
                    "category_completed": category,
                    "category_items": len(category_results),
                    "parallel_processing_stats": {
                        "successful_chunks": successful_chunks,
                        "failed_chunks": failed_chunks,
                        "total_chunks": len(chunks),
                        "processing_mode": "parallel"
                    },
                    "completed_category_items": category_results
                }
            )
        
        return category_results

    async def add_descriptions(
        self, 
        translated_data: dict, 
        session_id: Optional[str] = None
    ) -> DescriptionResult:
        """
        ç¿»è¨³ã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«è©³ç´°èª¬æ˜ã‚’è¿½åŠ ï¼ˆä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ç‰ˆï¼‰
        
        Args:
            translated_data: ç¿»è¨³ã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆé€²è¡ŒçŠ¶æ³é€šçŸ¥ç”¨ï¼‰
            
        Returns:
            DescriptionResult: è©³ç´°èª¬æ˜ç”Ÿæˆçµæœ
        """
        print("ğŸ“ Starting detailed description generation with OpenAI (Parallel Chunked Processing)...")
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if not self.is_available():
            return DescriptionResult(
                success=False,
                description_method="openai",
                error="OpenAI API is not available",
                metadata={
                    "error_type": "service_unavailable",
                    "suggestions": [
                        "Set OPENAI_API_KEY environment variable",
                        "Install openai package: pip install openai",
                        "Check OpenAI API access permissions",
                        "Verify internet connectivity"
                    ]
                }
            )
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if not self.validate_translated_data(translated_data):
            return DescriptionResult(
                success=False,
                description_method="openai",
                error="Invalid translated data",
                metadata={
                    "error_type": "invalid_input",
                    "suggestions": [
                        "Provide valid translated menu data",
                        "Ensure at least one category has menu items",
                        "Check translated data structure format"
                    ]
                }
            )
        
        try:
            final_menu = {}
            total_items = sum(len(items) for items in translated_data.values())
            
            print(f"ğŸ”¢ Total items to process: {total_items}")
            print(f"ğŸš€ Parallel processing enabled with max {settings.CONCURRENT_CHUNK_LIMIT} concurrent chunks")
            
            # ã‚«ãƒ†ã‚´ãƒªã®ä¸¦åˆ—å‡¦ç†ã‚‚å¯èƒ½ã«ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if settings.ENABLE_CATEGORY_PARALLEL and len(translated_data) > 1:
                print("ğŸŒŸ Category-level parallel processing enabled")
                
                # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                category_tasks = []
                for category, items in translated_data.items():
                    if items:  # ç©ºã§ãªã„ã‚«ãƒ†ã‚´ãƒªã®ã¿å‡¦ç†
                        task = self.process_category_parallel(category, items, session_id)
                        category_tasks.append((category, task))
                
                # ã‚«ãƒ†ã‚´ãƒªã‚’ä¸¦åˆ—ã§å‡¦ç†
                category_results = await asyncio.gather(
                    *[task for _, task in category_tasks], 
                    return_exceptions=True
                )
                
                # çµæœã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                for i, (category, _) in enumerate(category_tasks):
                    if i < len(category_results) and not isinstance(category_results[i], Exception):
                        final_menu[category] = category_results[i]
                    else:
                        print(f"âš ï¸ Category {category} processing failed")
                        final_menu[category] = []
                
                # ç©ºã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
                for category, items in translated_data.items():
                    if not items:
                        final_menu[category] = []
                        
            else:
                # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«é †æ¬¡å‡¦ç†ï¼ˆä½†ã—ãƒãƒ£ãƒ³ã‚¯å†…ã¯ä¸¦åˆ—ï¼‰
                for category, items in translated_data.items():
                    if not items:
                        final_menu[category] = []
                        continue
                    
                    # ã‚«ãƒ†ã‚´ãƒªå†…ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
                    category_results = await self.process_category_parallel(category, items, session_id)
                    final_menu[category] = category_results
            
            print(f"ğŸ‰ OpenAI Parallel Description Generation Complete: Added descriptions to {len(final_menu)} categories, {total_items} total items")
            
            # å‡¦ç†çµ±è¨ˆã‚’ç”Ÿæˆ
            statistics = self.extract_processing_statistics(translated_data, final_menu)
            statistics["processing_mode"] = "parallel_chunked"
            statistics["concurrent_chunk_limit"] = settings.CONCURRENT_CHUNK_LIMIT
            
            return DescriptionResult(
                success=True,
                final_menu=final_menu,
                description_method="openai",
                metadata={
                    "total_items": total_items,
                    "categories_processed": len(final_menu),
                    "provider": "OpenAI API",
                    "model": settings.OPENAI_MODEL,
                    "features": [
                        "parallel_chunked_processing",
                        "detailed_descriptions",
                        "cultural_context",
                        "tourist_friendly_language",
                        "real_time_progress",
                        "concurrent_execution"
                    ],
                    "processing_statistics": statistics
                }
            )
            
        except Exception as e:
            print(f"âŒ OpenAI Parallel Description Generation Failed: {e}")
            
            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            error_type = "unknown_error"
            suggestions = []
            
            if "rate limit" in str(e).lower():
                error_type = "rate_limit_exceeded"
                suggestions = [
                    "Reduce concurrent chunk limit",
                    "Wait before retrying",
                    "Check OpenAI API usage limits",
                    "Consider upgrading your OpenAI plan"
                ]
            elif "timeout" in str(e).lower():
                error_type = "api_timeout"
                suggestions = [
                    "Retry the request",
                    "Check internet connectivity",
                    "Reduce parallel processing load",
                    "Increase timeout settings"
                ]
            else:
                suggestions = [
                    "Check OPENAI_API_KEY is valid",
                    "Verify model access permissions", 
                    "Check OpenAI service status",
                    "Try reducing concurrent chunk limit"
                ]
            
            return DescriptionResult(
                success=False,
                description_method="openai",
                error=f"OpenAI parallel description generation error: {str(e)}",
                metadata={
                    "error_type": error_type,
                    "original_error": str(e),
                    "suggestions": suggestions,
                    "provider": "OpenAI API",
                    "model": settings.OPENAI_MODEL,
                    "processing_mode": "parallel_chunked"
                }
            )

def get_progress_function():
    """send_progressé–¢æ•°ã‚’å‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ï¼‰"""
    try:
        from app.services.realtime import send_progress
        return send_progress
    except ImportError:
        return None

def get_progress_function_stage4():
    """Stage4ç”¨ã®send_progressé–¢æ•°ã‚’å‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ï¼‰"""
    try:
        from app.services.realtime import send_progress
        return send_progress
    except ImportError:
        return None

async def get_description_progress_function():
    """è©³ç´°èª¬æ˜ç”¨ã®send_progressé–¢æ•°ã‚’å‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ï¼‰"""
    try:
        from app.services.realtime import send_progress
        return send_progress
    except ImportError:
        return None
