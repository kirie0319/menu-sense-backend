"""
éåŒæœŸç”»åƒç”Ÿæˆç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

from typing import Dict, List, Optional, Any
import logging
import time
from app.tasks.utils import (
    create_image_chunks, validate_menu_data, generate_job_id,
    save_job_info, get_job_info, calculate_job_progress, cleanup_job_data
)
from app.tasks.image_tasks import advanced_image_chunk_task, real_image_chunk_task
from app.core.config import settings

logger = logging.getLogger(__name__)

class AsyncImageManager:
    """éåŒæœŸç”»åƒç”Ÿæˆã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.manager_name = "AsyncImageManager"
        logger.info(f"{self.manager_name} initialized")
    
    def _validate_menu_data(self, final_menu: Dict[str, List[Dict]]) -> tuple[bool, str]:
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if not validate_menu_data(final_menu):
                return False, "Invalid menu data format"
            
            # ã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒã‚§ãƒƒã‚¯
            total_items = sum(len(items) for items in final_menu.values())
            
            if total_items == 0:
                return False, "No menu items found"
            
            # æ–°ã—ã„æŸ”è»Ÿãªåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if settings.UNLIMITED_PROCESSING:
                # ç„¡åˆ¶é™ãƒ¢ãƒ¼ãƒ‰ - åˆ¶é™ãªã—
                logger.info(f"ğŸš€ UNLIMITED_PROCESSING mode: Processing {total_items} items without limits")
                return True, f"Unlimited processing enabled for {total_items} items"
            
            # åˆ¶é™è¨ˆç®—
            max_allowed = self._calculate_max_items()
            
            if total_items > max_allowed:
                return False, f"Too many items ({total_items}). Maximum allowed: {max_allowed}"
            
            logger.info(f"ğŸ“Š Processing validation passed: {total_items}/{max_allowed} items")
            return True, f"Validation passed for {total_items} items"
            
        except Exception as e:
            logger.error(f"Menu validation error: {e}")
            return False, f"Validation error: {str(e)}"
    
    def _calculate_max_items(self) -> int:
        """å‹•çš„ã«æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—"""
        if settings.UNLIMITED_PROCESSING:
            return float('inf')  # ç„¡åˆ¶é™
        
        if settings.SCALE_WITH_WORKERS:
            # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã«å¿œã˜ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            calculated_max = settings.MAX_IMAGE_WORKERS * settings.ITEMS_PER_WORKER_RATIO
            logger.info(f"ğŸ”¢ Calculated max items: {settings.MAX_IMAGE_WORKERS} workers Ã— {settings.ITEMS_PER_WORKER_RATIO} ratio = {calculated_max}")
            return calculated_max
        else:
            # å›ºå®šåˆ¶é™
            return settings.MAX_ITEMS_PER_JOB
    

    
    def create_job_chunks(self, final_menu: Dict[str, List[Dict]]) -> List[Dict]:
        """
        ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¸ãƒ§ãƒ–ç”¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ
        
        Args:
            final_menu: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        """
        chunks = create_image_chunks(
            final_menu, 
            chunk_size=settings.IMAGE_PROCESSING_CHUNK_SIZE
        )
        
        logger.info(f"Created {len(chunks)} chunks for processing")
        return chunks
    
    def start_async_generation(
        self, 
        final_menu: Dict[str, List[Dict]], 
        session_id: Optional[str] = None
    ) -> tuple[bool, str, Optional[str]]:
        """
        éåŒæœŸç”»åƒç”Ÿæˆã‚’é–‹å§‹
        
        Args:
            final_menu: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            
        Returns:
            (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸, ã‚¸ãƒ§ãƒ–ID)
        """
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            is_valid, error_message = self._validate_menu_data(final_menu)
            if not is_valid:
                logger.error(f"Request validation failed: {error_message}")
                return False, error_message, None
            
            # ã‚¸ãƒ§ãƒ–IDç”Ÿæˆ
            job_id = generate_job_id()
            logger.info(f"Starting async image generation: job_id={job_id}")
            
            # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            chunks = self.create_job_chunks(final_menu)
            
            if not chunks:
                return False, "No chunks created from menu data", None
            
            # ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’ä½œæˆãƒ»ä¿å­˜
            job_data = {
                "job_id": job_id,
                "status": "starting",
                "total_chunks": len(chunks),
                "session_id": session_id,
                "created_at": time.time(),
                "menu_data": final_menu,
                "chunk_size": settings.IMAGE_PROCESSING_CHUNK_SIZE,
                "estimated_time": sum(len(items) for items in final_menu.values()) * 2,  # 2ç§’/ã‚¢ã‚¤ãƒ†ãƒ 
                "total_items": sum(len(items) for items in final_menu.values())
            }
            
            if not save_job_info(job_id, job_data):
                return False, "Failed to save job information", None
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã«é€ä¿¡
            task_ids = []
            
            # å®Ÿéš›ã®ç”»åƒç”Ÿæˆã‚’è¡Œã†ã‹ãƒ¢ãƒƒã‚¯å‡¦ç†ã«ã™ã‚‹ã‹ã‚’è¨­å®šã§åˆ¶å¾¡
            use_real_image_generation = getattr(settings, 'USE_REAL_IMAGE_GENERATION', True)
            selected_task = real_image_chunk_task if use_real_image_generation else advanced_image_chunk_task
            task_type = "real_image" if use_real_image_generation else "mock_image"
            
            logger.info(f"Using {task_type} generation for job {job_id}")
            
            for chunk in chunks:
                chunk_info = {
                    "chunk_id": chunk["chunk_id"],
                    "total_chunks": chunk["total_chunks"],
                    "category": chunk["category"]
                }
                
                try:
                    task = selected_task.delay(
                        chunk["items"], 
                        chunk_info, 
                        job_id
                    )
                    task_ids.append(task.id)
                    logger.info(f"Queued {task_type} chunk {chunk['chunk_id']}: task_id={task.id}")
                    
                except Exception as e:
                    logger.error(f"Failed to queue chunk {chunk['chunk_id']}: {e}")
                    continue
            
            if not task_ids:
                cleanup_job_data(job_id)
                return False, "Failed to queue any tasks", None
            
            # ã‚¸ãƒ§ãƒ–æƒ…å ±ã«ã‚¿ã‚¹ã‚¯IDã‚’è¿½åŠ 
            job_data["task_ids"] = task_ids
            job_data["status"] = "queued"
            job_data["queued_at"] = time.time()
            
            if not save_job_info(job_id, job_data):
                logger.warning(f"Failed to update job info with task IDs: {job_id}")
            
            logger.info(f"Async image generation started: job_id={job_id}, chunks={len(chunks)}, tasks={len(task_ids)}")
            return True, f"Image generation started with {len(chunks)} chunks", job_id
            
        except Exception as e:
            logger.error(f"Failed to start async image generation: {e}")
            return False, f"Internal error: {str(e)}", None
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """
        ã‚¸ãƒ§ãƒ–ã®ç¾åœ¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—
        
        Args:
            job_id: ã‚¸ãƒ§ãƒ–ID
            
        Returns:
            ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±
        """
        try:
            # åŸºæœ¬ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’å–å¾—
            job_info = get_job_info(job_id)
            if not job_info:
                return {
                    "found": False,
                    "error": "Job not found",
                    "job_id": job_id
                }
            
            # é€²è¡ŒçŠ¶æ³ã‚’è¨ˆç®—
            progress_info = calculate_job_progress(job_id)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ§‹ç¯‰
            response = {
                "found": True,
                "job_id": job_id,
                "status": progress_info.get("status", "unknown"),
                "progress_percent": progress_info.get("progress_percent", 0),
                "total_chunks": job_info.get("total_chunks", 0),
                "completed_chunks": progress_info.get("completed_chunks", 0),
                "failed_chunks": progress_info.get("failed_chunks", 0),
                "created_at": job_info.get("created_at", 0),
                "session_id": job_info.get("session_id"),
                "total_items": job_info.get("total_items", 0),
                "estimated_time": job_info.get("estimated_time", 0)
            }
            
            # å®Œäº†æ™‚ã¯çµæœã‚‚å«ã‚ã‚‹
            if response["status"] in ["completed", "partial_completed"]:
                chunk_results = progress_info.get("chunk_results", {})
                
                # çµæœã‚’çµ±åˆ
                final_images = {}
                total_successful_images = 0
                
                for chunk_id, chunk_result in chunk_results.items():
                    if chunk_result.get("status") == "completed":
                        category = chunk_result.get("category", "Unknown")
                        results = chunk_result.get("results", [])
                        
                        if category not in final_images:
                            final_images[category] = []
                        
                        final_images[category].extend(results)
                        total_successful_images += len([r for r in results if r.get("generation_success")])
                
                response.update({
                    "images_generated": final_images,
                    "total_images": total_successful_images,
                    "completed_at": max(
                        (chunk.get("completed_at", 0) for chunk in chunk_results.values() 
                         if chunk.get("status") == "completed"), 
                        default=time.time()
                    )
                })
            
            return response
            
        except Exception as e:
            logger.error(f"Failed to get job status: {e}")
            return {
                "found": False,
                "error": f"Status retrieval error: {str(e)}",
                "job_id": job_id
            }
    
    def cancel_job(self, job_id: str) -> tuple[bool, str]:
        """
        ã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        
        Args:
            job_id: ã‚¸ãƒ§ãƒ–ID
            
        Returns:
            (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        try:
            job_info = get_job_info(job_id)
            if not job_info:
                return False, "Job not found"
            
            # ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã‚’ç¢ºèª
            current_status = job_info.get("status", "unknown")
            if current_status in ["completed", "failed", "cancelled"]:
                return False, f"Cannot cancel job in {current_status} state"
            
            # ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼ˆCeleryã‚¿ã‚¹ã‚¯ã®åœæ­¢ï¼‰
            task_ids = job_info.get("task_ids", [])
            cancelled_tasks = 0
            
            for task_id in task_ids:
                try:
                    # Celeryã‚¿ã‚¹ã‚¯ã‚’å–ã‚Šæ¶ˆã—
                    from app.tasks.celery_app import celery_app
                    celery_app.control.revoke(task_id, terminate=True)
                    cancelled_tasks += 1
                except Exception as e:
                    logger.warning(f"Failed to cancel task {task_id}: {e}")
            
            # ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            job_info["status"] = "cancelled"
            job_info["cancelled_at"] = time.time()
            job_info["cancelled_tasks"] = cancelled_tasks
            
            save_job_info(job_id, job_info)
            
            logger.info(f"Job cancelled: job_id={job_id}, cancelled_tasks={cancelled_tasks}")
            return True, f"Job cancelled successfully ({cancelled_tasks} tasks stopped)"
            
        except Exception as e:
            logger.error(f"Failed to cancel job: {e}")
            return False, f"Cancellation error: {str(e)}"
    
    def cleanup_old_jobs(self, max_age_hours: int = 24) -> tuple[int, int]:
        """
        å¤ã„ã‚¸ãƒ§ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            max_age_hours: æœ€å¤§ä¿æŒæ™‚é–“ï¼ˆæ™‚é–“ï¼‰
            
        Returns:
            (å‰Šé™¤ä»¶æ•°, ã‚¨ãƒ©ãƒ¼ä»¶æ•°)
        """
        try:
            # å®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼ˆå®Ÿéš›ã«ã¯Redisã®ã‚­ãƒ¼ã‚’åˆ—æŒ™ã—ã¦å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼‰
            logger.info(f"Cleanup requested for jobs older than {max_age_hours} hours")
            # ã“ã“ã§ã¯å®Ÿè£…ã‚’çœç•¥
            return 0, 0
            
        except Exception as e:
            logger.error(f"Failed to cleanup old jobs: {e}")
            return 0, 1
    
    def get_manager_stats(self) -> Dict[str, Any]:
        """
        ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±
        """
        return {
            "manager_name": self.manager_name,
            "config": {
                "chunk_size": settings.IMAGE_PROCESSING_CHUNK_SIZE,
                "max_workers": settings.MAX_IMAGE_WORKERS,
                "job_timeout": settings.IMAGE_JOB_TIMEOUT,
                "async_enabled": settings.ASYNC_IMAGE_ENABLED
            },
            "features": [
                "async_image_generation",
                "chunk_based_processing", 
                "redis_progress_tracking",
                "job_cancellation",
                "real_time_status"
            ]
        }

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_async_manager_instance = None

def get_async_manager() -> AsyncImageManager:
    """AsyncImageManagerã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _async_manager_instance
    if _async_manager_instance is None:
        _async_manager_instance = AsyncImageManager()
    return _async_manager_instance
